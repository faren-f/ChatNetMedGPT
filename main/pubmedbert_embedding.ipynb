{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c25f26-f543-4677-924b-4e4102db5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646c28c-3e92-4bc7-be43-dfde8a26dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/parameters.json\", 'r') as file:\n",
    "    param = json.load(file)\n",
    "nodes = pd.read_csv(os.path.join(param['files']['data_dir'], \"nodes_snake.csv\"), sep= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc086e2-9bcb-42eb-bbaf-bde3040461ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "model = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "model.eval()\n",
    "model.to(\"cuda\")  # or \"cpu\" if no GPU\n",
    "\n",
    "\n",
    "def meanpooling(outputs, attention_mask):\n",
    "    token_embeddings = outputs.last_hidden_state          # [B, T, H]\n",
    "    mask = attention_mask.unsqueeze(-1).float()           # [B, T, 1]\n",
    "    summed = (token_embeddings * mask).sum(dim=1)         # [B, H]\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)              # [B, 1]\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "def node_embedding(list_nodes_sentence, batch_size=32):\n",
    "    all_embs = []\n",
    "\n",
    "    for start in range(0, len(list_nodes_sentence), batch_size):\n",
    "        batch = list_nodes_sentence[start : start + batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        emb = meanpooling(outputs, inputs[\"attention_mask\"])\n",
    "        all_embs.append(emb.cpu())\n",
    "\n",
    "    return torch.cat(all_embs, dim=0)   # [N, 768]\n",
    "\n",
    "\n",
    "# ------ your data ------\n",
    "# list_nodes_sentence: list[str], one text per node\n",
    "# all_node_names: list[str], same order as list_nodes_sentence\n",
    "\n",
    "emb_all = node_embedding(list_nodes_sentence, batch_size=32)  # [N, 768]\n",
    "\n",
    "# save embeddings + node names + index mapping\n",
    "torch.save(emb_all, \"node_emb_all.pt\")\n",
    "with open(\"all_node_names.json\", \"w\") as f:\n",
    "    json.dump(all_node_names, f)\n",
    "name2id = {name: i for i, name in enumerate(all_node_names)}\n",
    "with open(\"name2id.json\", \"w\") as f:\n",
    "    json.dump(name2id, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
