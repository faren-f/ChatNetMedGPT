import torch
import torch.nn as nn
import torch.nn.functional as F 

# Create a transformer model for masked word prediction
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, N_encoder_layers, seq_len, device, feat, nodes, entity, relation_mask_index, mask_token_id=None, pos_emb=None):
        super(TransformerModel, self).__init__()

        self.device = device
        self.vocab_size = vocab_size
        self.mask_token_id = mask_token_id
        self.nodes = nodes
        self.entity = entity
        self.relation_mask_index = relation_mask_index

        ## MLP configuration for node attributes    
        self.feat_all = nn.ParameterDict()
        self.fc_all = nn.ModuleDict()
        for k, v in feat.items():
            if len(v) > 1:                      # nodes that have two or more type of attributes e.g., protein  
                for k1, v1 in v.items():
                    name = f"{k}|{k1}"  # attach parent name to subkey
                    self.feat_all[name] = nn.Parameter(v1.to(device), requires_grad = False)
                    self.fc_all[name] = nn.Sequential(
                        nn.Linear(v1.shape[1], d_model, device=device)
                    )
                    
            else:
                v1 = list(v.values())[0]
                param = nn.Parameter(v1.to(device))
                if list(v.keys())[0] == 'random':  # nodes that do not have attr e.g., anatomy
                    param.requires_grad = True
                elif list(v.keys())[0] == 'fixed':    # relations and mask
                    param.requires_grad = False
                else:                                # nodes that have one type of attributes e.g., disease
                    param.requires_grad = False
                    
                    
                self.feat_all[k] = param
                self.fc_all[k] = nn.Sequential(
                        nn.Linear(v1.shape[1], d_model, device=device)
                    )


        self.pos_emb = pos_emb # positional embedding
        if pos_emb == 'fixed':
            self.pos_encoding = self.create_positional_encoding(seq_len, d_model).to(device)    # Positional encoding (computed dynamically)
        elif pos_emb == 'learnable':
            self.pos_embedding = nn.Embedding(seq_len, d_model).to(device)
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, activation='gelu').to(device),
            num_layers = N_encoder_layers
            )

        
        self.fc = nn.Linear(d_model, vocab_size+1).to(device)  # Output layer: Predict all vocab including [MASK]
        
        # binary classification layer 
        self.classifire = nn.Sequential(
            nn.Linear(d_model * 2, d_model),  # considering only head and tail for LP unless it should be d_model*3
            nn.ReLU(),
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Linear(d_model//2, d_model//3),
            nn.ReLU(),
            nn.Linear(d_model//3, 1)
        ).to(device)

        
    def forward(self, x):  
        x = self.compute_embedding(x) + self.get_x_pos_emb(x)
        x = self.transformer(x.permute(1, 0, 2))    
        x = self.fc(x.permute(1, 0, 2))
        return x

    def classify(self, x):   
        x = self.compute_embedding(x) + self.get_x_pos_emb(x)
        x = self.transformer(x.permute(1, 0, 2))
        x = x.permute(1, 0, 2)
        # x = torch.concat([x[:,0,:], x[:,1,:], x[:,2,:]], dim=1)
        # x = x[:, :3, :]              # shape: [batch, 3, hidden_dim]
        x = x[:, 0:3:2, :]              # shape: [batch, 2, hidden_dim]    # only considering head and tail for LP
        x = x.reshape(x.size(0), -1) # shape: [batch, 2 * hidden_dim]
        x = self.classifire(x)
        return x

    def get_embeddings(self, tokens, select = 'raw'):
        with torch.no_grad():
            if select == 'raw':
                embeddings = self.compute_embedding(tokens)
            elif select == 'transformer':
                x = self.compute_embedding(tokens) + self.get_x_pos_emb(tokens)
                x = self.transformer(x.permute(1, 0, 2))
                embeddings = x.permute(1, 0, 2)
            else:
                raise(f'invalid "select" option: {select}')
        return embeddings

    # when we use node attr we generate embedding here with trainable parameters 
    def compute_embedding(self, tokens):
        W_list = []
        Indices = []
        for t in self.entity:
            matching_keys = [k for k in self.feat_all.keys() if k.split('|')[0]==t]
            tensors = [self.fc_all[k](self.feat_all[k]) for k in matching_keys]
            merged_tensor = torch.stack(tensors, dim=0).sum(dim=0)  # or use torch.mean(...), etc.
            W_list.append(merged_tensor)
            indices = list(self.nodes.loc[self.nodes['node_type'] == t,'node_index'])
            if len(indices)>0:
                Indices = Indices + indices
            elif len(indices) == 0:
                index = list(self.relation_mask_index.loc[self.relation_mask_index['relation'] == t, 'z_index'])
                Indices = Indices + index

        W = torch.cat(W_list, dim=0).to(self.device)
        entity_indices = torch.tensor(Indices)
        
        # Get the sorted indices (i.e., the order to sort by)
        sorted_order = torch.argsort(entity_indices)
        
        # Apply the sorted order to both the indices and the tensor W
        sorted_W = W[sorted_order]

        return F.embedding(tokens, sorted_W)

    def create_positional_encoding(self, seq_len, d_model):
        """Generates positional encoding dynamically"""
        pe = torch.zeros(seq_len, d_model)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)

    def get_x_pos_emb(self, x):
        if self.pos_emb == 'fixed':
            x_pos_emb = self.pos_encoding[:x.shape[1], :]
        elif self.pos_emb == 'learnable':
            positions = torch.arange(0, x.shape[1], device=x.device).unsqueeze(0)
            x_pos_emb = self.pos_embedding(positions)
        else:
            x_pos_emb = 0
        return x_pos_emb





